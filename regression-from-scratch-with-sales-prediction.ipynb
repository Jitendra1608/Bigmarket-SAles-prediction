{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION From Scratch With SALES PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://drive.google.com/uc?id=1-6z0sZc9YrK_czjy8mBQuxBj3wdD01-V' width=800 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this Notebook we will Learn:-\n",
    "* Basic EDA.\n",
    "* Feature Engineering\n",
    "* Dealing with missing values.\n",
    "* Aplly Scaling on Feature matrix.\n",
    "* Dealing with Categorical Dataset.\n",
    "* Dimensionality Reduction (PCA) .\n",
    "* K-Cross validation to check accuracy.\n",
    "* Multi-linear Regression\n",
    "* Random Forest Regressor\n",
    "* Polynomial Regression\n",
    "* Prediction on new Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e25df6f246c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minit_notebook_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_plotlyjs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcufflinks\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0minit_notebook_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnected\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotly.offline import init_notebook_mode, download_plotlyjs, iplot\n",
    "import cufflinks as cf\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "print()\n",
    "print(\"The files in the dataset are:-\")\n",
    "from subprocess import check_output\n",
    "print(check_output(['ls','../input']).decode('utf'))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Importing the datasets\n",
    "df_train = pd.read_csv('../input/Train.csv')\n",
    "df_test = pd.read_csv('../input/Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC ANALYSIS AND FEATURES ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1). Removing Unwanted Columns/Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_train.drop(labels=['Item_Identifier', 'Outlet_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "    df_test.drop(labels=['Item_Identifier', 'Outlet_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2). Getting Information about Null values,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df_train.isnull().sum().reset_index()\n",
    "temp_df['Percentage'] = (temp_df[0]/len(df_train))*100\n",
    "temp_df.columns = ['Column Name', 'Number of null values', 'Null values in percentage']\n",
    "print(f\"The length of dataset is \\t {len(df_train)}\")\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So it is clear that we do not have to remove null values, as they 28% and 17% in the Outlet_Size and Item_Weight Columns respectively.\n",
    "* Null values are in less quantity.\n",
    "* We will replace them later with thier mean or mode values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3). Making Correction in 'Item_Fat_Content' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    if x in ['low fat', 'LF']: \n",
    "        return 'Low Fat'\n",
    "    elif x=='reg':\n",
    "        return 'Regular'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "df_train['Item_Fat_Content'] = df_train['Item_Fat_Content'].apply(convert)\n",
    "df_test['Item_Fat_Content'] = df_train['Item_Fat_Content'].apply(convert)\n",
    "\n",
    "print(f\"Now Unique values in this column in Train Set are\\t  {df_train['Item_Fat_Content'].unique()} \")\n",
    "print(f\"Now Unique values in this column in Test Set are\\t  {df_test['Item_Fat_Content'].unique()} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4). Dealing with the Missing Values in Categorical type column i.e. 'Outlet_Size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the values\n",
    "count = df_train['Outlet_Size'].value_counts().reset_index()\n",
    "count.iplot(kind='bar', color='deepskyblue', x='index', y='Outlet_Size', title='High VS Mediun VS Small',\n",
    "           xTitle='Size', yTitle='Frequency')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will remove the missing values from 'Medium' in both Training set and Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Outlet_Size'].fillna(value='Medium', inplace= True)\n",
    "df_test['Outlet_Size'].fillna(value='Medium', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION WITH REGRESSION MODELS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us Import the Important Libraries  to train our Model for Machine Learning \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder # To deal with Categorical Data in Target Vector.\n",
    "from sklearn.model_selection import train_test_split  # To Split the dataset into training data and testing data.\n",
    "from sklearn.model_selection import cross_val_score   # To check the accuracy of the model.\n",
    "from sklearn.preprocessing import Imputer   # To deal with the missing values\n",
    "from sklearn.preprocessing import StandardScaler   # To appy scaling on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create feature matrix and Target Vector.\n",
    "x_train = df_train.iloc[:, :-1].values    # Features Matrix\n",
    "y_train = df_train.iloc[:,-1].values   # Target Vector\n",
    "x_test = df_test.values    # Features Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1). Dealing with Missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer()\n",
    "x_train[:,[0]] = imputer.fit_transform(x_train[:,[0]])\n",
    "x_test[:,[0]] = imputer.fit_transform(x_test[:,[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2). Dealing With the Categorical Values in Features/Columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder_x = LabelEncoder()\n",
    "x_train[:, 1 ] = labelencoder_x.fit_transform(x_train[:,1 ])\n",
    "x_train[:, 3 ] = labelencoder_x.fit_transform(x_train[:,3 ])\n",
    "x_train[:, 5 ] = labelencoder_x.fit_transform(x_train[:,5 ])\n",
    "x_train[:, 6 ] = labelencoder_x.fit_transform(x_train[:,6 ])\n",
    "x_train[:, 7 ] = labelencoder_x.fit_transform(x_train[:,7 ])\n",
    "\n",
    "\n",
    "#this is need to done when we have more than two categorical values.\n",
    "onehotencoder_x = OneHotEncoder(categorical_features=[3,5,6,7]) \n",
    "x_train = onehotencoder_x.fit_transform(x_train).toarray()\n",
    "\n",
    "# Let's apply same concept on test set.\n",
    "x_test[:, 1 ] = labelencoder_x.fit_transform(x_test[:,1 ])\n",
    "x_test[:, 3 ] = labelencoder_x.fit_transform(x_test[:,3 ])\n",
    "x_test[:, 5 ] = labelencoder_x.fit_transform(x_test[:,5 ])\n",
    "x_test[:, 6 ] = labelencoder_x.fit_transform(x_test[:,6 ])\n",
    "x_test[:, 7 ] = labelencoder_x.fit_transform(x_test[:,7 ])\n",
    "\n",
    "\n",
    "#this is need to done when we have more than two categorical values.\n",
    "onehotencoder_x = OneHotEncoder(categorical_features=[3,5,6,7]) \n",
    "x_test = onehotencoder_x.fit_transform(x_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3). Now time to Apply Feature Scaling on Feature matrix ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X=StandardScaler()\n",
    "x_train=sc_X.fit_transform(x_train)\n",
    "x_test = sc_X.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4). DIMENSIONALITY REDUCTION\n",
    "* We are doing this to reduce the number of dimensions/features in the dataset.\n",
    "* The features which have less effect on the prediction , we will remove those features.\n",
    "* It also boosts the process.\n",
    "* It saves time.\n",
    "* Here we will use Principal Component Analysis (PCA) with 'rbf' kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=None)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.fit_transform(x_test)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we will take n_component = 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=25)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5). Apply Multi-Linear Regression Model, Polynomial Regression and Random Forest Model and compare thier accuracy and pick the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-linear regression Model.\n",
    "regressor_multi = LinearRegression()\n",
    "regressor_multi.fit(x_train,y_train)\n",
    "\n",
    "# Let us check the accuray\n",
    "accuracy = cross_val_score(estimator=regressor_multi, X=x_train, y=y_train,cv=10)\n",
    "print(f\"The accuracy of the Multi-linear Regressor Model is \\t {accuracy.mean()}\")\n",
    "print(f\"The deviation in the accuracy is \\t {accuracy.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Random Forest Model.\n",
    "regressor_random = RandomForestRegressor(n_estimators=100,)\n",
    "regressor_random.fit(x_train,y_train)\n",
    "\n",
    "# Let us check the accuray\n",
    "accuracy = cross_val_score(estimator=regressor_random, X=x_train, y=y_train,cv=10)\n",
    "print(f\"The accuracy of the Random Forest Model is \\t {accuracy.mean()}\")\n",
    "print(f\"The deviation in the accuracy is \\t {accuracy.std()}\") \"\"\"\n",
    "\n",
    "print(\"Here accuray is 53% with deviation of 3%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Fitting polynomial regression to dataset\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_reg=PolynomialFeatures(degree=4) #These 3 steps are to convert X matrix into X polynomial\n",
    "x_poly=poly_reg.fit_transform(x_train) #matrix. \n",
    "regressor_poly=LinearRegression()\n",
    "regressor_poly.fit(x_poly,y_train)\n",
    "\n",
    "# Let us check the accuray\n",
    "accuracy = cross_val_score(estimator=regressor_poly, X=x_train, y=y_train,cv=10)\n",
    "print(f\"The accuracy of the Polynomial Regression Model is \\t {accuracy.mean()}\")\n",
    "print(f\"The deviation in the accuracy is \\t {accuracy.std()}\")\n",
    "\"\"\"\n",
    "print(\"Here accuracy is 55% with deviation of 2%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### observation:-\n",
    "* As the accuracy of Multi-linear regression Model is the best one.\n",
    "* Multi-linear Regression Model takes less time as compare to Random forest and Polynomial regression Models.\n",
    "* We will choose Multi-linear regression Model.\n",
    "* Here we are getting the accuracy of 55% and deviation of 2%, means in future if we mak eprediction on new values then we will get the accuracy in range 53% to 57%.\n",
    "* We are getting low accuracy due to less quantity of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us make Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor_multi.predict(x_test)\n",
    "\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ============================================================================\n",
    "### ============================================================================\n",
    "### ============================================================================\n",
    "### ============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IF THIS KERNEL IS HELPFUL, THEN PLEASE UPVOTE.\n",
    "<img src='https://drive.google.com/uc?id=1LBdaJj2pTM0cq9PY6k70RaGfUFDakUzG' width=500 >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
